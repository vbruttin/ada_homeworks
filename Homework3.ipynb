{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA 2018 - Homework 3\n",
    "\n",
    "\n",
    "\n",
    "## Undestanding the StackOverflow community\n",
    "\n",
    "\n",
    "Deadline: Nov 7th 2018, 23:59:59\n",
    "\n",
    "Submission link: Check channel homework-3-public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackOverflow is the most popular programming-related Q&A website. It serves as a platform for users to ask and answer questions and to vote questions and answers up or down. Users of StackOverflow can earn reputation points and \"badges\"; for example, a person is awarded 10 reputation points for receiving an \"up\" vote on an answer given to a question, and 5 points for the \"up\" vote on a question asked. Also, users receive badges for their valued contributions, which represents a kind of gamification of the traditional Q&A site. \n",
    "\n",
    "[Learn more about StackOverflow on Wikipedia](https://en.wikipedia.org/wiki/Stack_Overflow)\n",
    "\n",
    "----\n",
    "\n",
    "Dataset link:\n",
    "\n",
    "https://drive.google.com/open?id=1POlGjqzw9v_pZ_bUnXGihOgk45kbvNjB\n",
    "\n",
    "http://iccluster053.iccluster.epfl.ch/Posts.json.zip (mirror 1)\n",
    "\n",
    "https://iloveadatas.com/datasets/Posts.json.zip (mirror 2)\n",
    "\n",
    "Dataset description:\n",
    "\n",
    "* **Id**: Id of the post\n",
    "* **CreationDate**: Creation date of the post (String format)\n",
    "* **PostTypeId**: Type of post (Question = 1, Answer = 2)\n",
    "* **ParentId**: The id of the question. Only present if PostTypeId = 2\n",
    "* **Score**: Points assigned by the users\n",
    "* **Tags**: Tags of the question. Only present if PostTypeId = 1\n",
    "* **Title**: Only present if PostTypeId = 1\n",
    "* **ViewCount**: Only present if PostTypeId = 1\n",
    "\n",
    "The dataset format is JSON. Here are examples of a question and an answer:\n",
    "\n",
    "Question:\n",
    "```json\n",
    "{\n",
    "    \"Id\": 10130734,\n",
    "    \"CreationDate\": \"2012-04-12T19:51:25.793+02:00\",\n",
    "    \"PostTypeId\": 1,\n",
    "    \"Score\": 4,\n",
    "    \"Tags\": \"<python><pandas>\",\n",
    "    \"Title\": \"Best way to insert a new value\",\n",
    "    \"ViewCount\": 3803\n",
    "}\n",
    "```\n",
    "\n",
    "Answer:\n",
    "```json\n",
    "{  \n",
    "   \"CreationDate\":\"2010-10-26T03:19:05.063+02:00\",\n",
    "   \"Id\":4020440,\n",
    "   \"ParentId\":4020214,\n",
    "   \"PostTypeId\":2,\n",
    "   \"Score\":1\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "Useful resources:\n",
    "\n",
    "**Spark SQL, DataFrames and Datasets Guide**\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "**Database schema documentation for the public data dump**\n",
    "\n",
    "https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** Use Spark where possible. Some computations can take more than 10 minutes on a common notebook. Consider to save partial results on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "DATA_FOLDER = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Convert the dataset to a more convenient format\n",
    "As a warm-up task (and to avoid to warm up your laptop too much), load the dataset into a Spark dataframe, show the content, and save it in the _Parquet_ format. Use this step to convert the fields to a more convenient form.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. How many questions have been asked on StackOverflow?\n",
    "2. How many answers have been given?\n",
    "3. What is the percentage of questions with a score of 0?\n",
    "\n",
    "**Hint:** The next tasks involve a time difference. Consider storing time in numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload in a more beautiful way\n",
    "# data_stack = spark.read.json(requests.get(\"https://iloveadatas.com/datasets/Posts.json.zip\"))\n",
    "#May not work beacause of the .zip -> TODO : find how or juste assume that we load into our folder the data set\n",
    "\n",
    "#Upload data if they are in the folder ./data/\n",
    "data_stack = spark.read.json(DATA_FOLDER + 'Posts.json')\n",
    "data_stack.write.parquet(\"data_stack.parquet\")\n",
    "data_stack = spark.read.parquet(\"data_stack.parquet\")\n",
    "data_stack_rdd = data_stack.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "|        CreationDate|      Id|ParentId|PostTypeId|Score|                Tags|               Title|ViewCount|\n",
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "|2017-08-17T16:20:...|45740344|45740224|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740346|45739185|         2|    1|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740348|    null|         1|    2|<flash><react-nat...|Is it possible to...|      143|\n",
      "|2017-08-17T16:20:...|45740350|45739102|         2|    1|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740352|42473616|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740354|45668191|         2|    1|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740355|    null|         1|    1|<postgresql><form...|Remove trailing z...|      444|\n",
      "|2017-08-17T16:21:...|45740357|44484201|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740358|    null|         1|    0|<python><websocke...|Python websockets...|      280|\n",
      "|2017-08-17T16:21:...|45740359|19922107|         2|    4|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740361|45740273|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740363|    null|         1|    0|<facebook><facebo...|Image meta tag no...|       97|\n",
      "|2017-08-17T16:21:...|45740367|45725024|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740369|45724556|         2|    1|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740370|45739756|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740371|    null|         1|    1|    <mongodb><shell>|Using Mongo-cli t...|      185|\n",
      "|2017-08-17T16:21:...|45740372|    null|         1|    0|            <impala>|Impact of \"INVALI...|      327|\n",
      "|2017-08-17T16:21:...|45740374|45739780|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740375|45721970|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:21:...|45740376|45740035|         2|    1|                null|                null|     null|\n",
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stack.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of question in StackOverflow : 15647060\n"
     ]
    }
   ],
   "source": [
    "number__of_question = data_stack_rdd.filter(lambda x : x.PostTypeId == 1).count()\n",
    "print(\"The number of question in StackOverflow : {}\".format(number__of_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of answer in StackOverflow : 25192772\n"
     ]
    }
   ],
   "source": [
    "number__of_answer = data_stack_rdd.filter(lambda x : x.PostTypeId == 2).count()\n",
    "print(\"The number of answer in StackOverflow : {}\".format(number__of_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of question with a null score in StackOverflow : 0.4654\n"
     ]
    }
   ],
   "source": [
    "number_of_question_score_0 = data_stack_rdd.filter(lambda x : (x.PostTypeId == 1) and (x.Score == 0)).count()\n",
    "print(\"The proportion of question with a null score in StackOverflow : {}\"\\\n",
    "      .format(str(number_of_question_score_0/number__of_question)[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Task B\n",
    "data_stack.select(\"Tags\").write.save(\"Tags.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Task C\n",
    "data_stack.select(\"Score\", \"ViewCount\", \"Tags\").write.save(\"Score_View_Tags.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Load the dataset from the Parquet file for the next tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B: What are the 10 most popular tags?\n",
    "\n",
    "What are the most popular tags in StackOverflow? Use Spark to extract the information you need, and answer the following questions with Pandas and Matplotlib (or Seaborn):\n",
    "\n",
    "1. What is the proportion of tags that appear in fewer than 100 questions?\n",
    "2. Plot the distribution of the tag counts using an appropriate representation.\n",
    "3. Plot a bar chart with the number of questions for the 10 most popular tags.\n",
    "\n",
    "For each task describe your findings briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have to sample?\n",
    "df_sampled = data_stack.sample(False, 0.0001, 42)\n",
    "rdd_posts = df_sampled.rdd\n",
    "rdd_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_tags = data_stack_rdd.filter(lambda r: r[5] != None).map(lambda r: (r[5][1:-1].split('><')))\n",
    "\n",
    "#if with sample\n",
    "#rdd_tags = rdd_posts.filter(lambda r: r[5] != None).map(lambda r: (r[5][1:-1].split('><')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>javascript</th>\n",
       "      <td>1535788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>1340350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c#</th>\n",
       "      <td>1158041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>php</th>\n",
       "      <td>1049934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>929698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "javascript  1535788\n",
       "java        1340350\n",
       "c#          1158041\n",
       "php         1049934\n",
       "python       929698"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tags2 = rdd_tags.countByKey()\n",
    "df_tags = pd.DataFrame.from_dict(rdd_tags2, orient='index')\n",
    "df_tags = df_tags.rename(index=str, columns={0: \"count\"})\n",
    "df_tags = df_tags.sort_values('count',ascending=False)\n",
    "df_tags.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of tags appearing in fewer than 100 questions : 90.05 %\n"
     ]
    }
   ],
   "source": [
    "total_tags = len(df_tags)\n",
    "df_tags_less_100 = df_tags[df_tags['count'] < 100]\n",
    "total_tags_less_100 = len(df_tags_less_100)\n",
    "print('Proportion of tags appearing in fewer than 100 questions : {} %'.format(str(total_tags_less_100*100/total_tags)[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C: View-score relation\n",
    "\n",
    "We want to investigate the correlation between the view count and the score of questions.\n",
    "\n",
    "1. Get the view count and score of the questions with tag ```random-effects``` and visualize the relation between these two variables using an appropriate plot.\n",
    "2. Are these two variables correlated? Use the Pearson coefficient to validate your hypothesis. Discuss your findings in detail.\n",
    "\n",
    "**Hint:** Inspect the data visually before drawing your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D: What are the tags with the fastest first answer?\n",
    "\n",
    "What are the tags that have the fastest response time from the community? We define the response time as the difference in seconds between the timestamps of the question and of the first answer received.\n",
    "\n",
    "1. Get the response time for the first answer of the questions with the tags ```python``` and ```java```.\n",
    "2. Plot the two distributions in an appropriate format. What do you observe? Describe your findings and discuss the following distribution properties: mean, median, standard deviation.\n",
    "3. We believe that the response time is lower for questions related to Python (compare to Java). Contradict or confirm this assumption by estimating the proper statistic with bootstrapping. Visualize the 95% confidence intervals with box plots and describe your findings.\n",
    "3. Repeat the first analysis (D1) by using the proper statistic to measure the response time for the tags that appear at least 5000 times. Plot the distribution of the 10 tags with the fastest response time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task E: What's up with PySpark?\n",
    "The number of questions asked regarding a specific topic reflect the publicâ€™s interest on it. We are interested on the popularity of PySpark. Compute and plot the number of questions with the ```pyspark``` tag for 30-day time intervals. Do you notice any trend over time? Is there any correlation between time and number of questions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
